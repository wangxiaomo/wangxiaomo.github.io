
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>数据聚类 - 小默的研究中心</title>
  <meta name="author" content="wangxiaomo">

  
  <meta name="description" content="数据聚类就是从各种不同的来源中构造算法所需的数据.聚类时常被用在数据量很大的应用中.
这里面首先有两个概念:监督学习和无监督学习.
利用样本输入和期望输出来学习如何预测的技术被称为监督学习.比如:神经网络,决策树,向量支持机,以及贝叶斯过滤.采用这些方法的应用程序, &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://wangxiaomo.github.com/blog/2011/07/13/%E6%95%B0%E6%8D%AE%E8%81%9A%E7%B1%BB/">
  <link href="/oct/favicon.png" rel="icon">
  <link href="/oct/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script src="/oct/javascripts/modernizr-2.0.js"></script>
  <script src="/oct/javascripts/ender.js"></script>
  <script src="/oct/javascripts/octopress.js" type="text/javascript"></script>
  <link href="/blog/atom.xml" rel="alternate" title="小默的研究中心" type="application/atom+xml">
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  

</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/oct/">小默的研究中心</a></h1>
  
    <h2>Perl | PHP | Python...技术宅.微博控</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/blog/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="http://google.com/search" method="get">
  <fieldset role="search">
    <input type="hidden" name="q" value="site:wangxiaomo.github.com" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/oct/">Blog</a></li>
  <li><a href="/oct/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      <h1 class="entry-title">数据聚类</h1>
    
    
      <p class="meta">
        








  


<time datetime="2011-07-13T03:08:00+08:00" pubdate data-updated="true">Jul 13<span>th</span>, 2011</time>
        
      </p>
    
  </header>


<div class="entry-content"><p>数据聚类就是从各种不同的来源中构造算法所需的数据.聚类时常被用在数据量很大的应用中.
这里面首先有两个概念:监督学习和无监督学习.
利用样本输入和期望输出来学习如何预测的技术被称为监督学习.比如:神经网络,决策树,向量支持机,以及贝叶斯过滤.采用这些方法的应用程序,会通过检查一组输入和期望的输出来进行&#8221;学习&#8221;.当我们想要利用这些方法中的任何一种来提取信息时,我们可以传入一组输入,然后期望应用程序能够根据其此前学到的知识来产生一个输出.
聚类是无监督学习.与神经网络或决策树不同,无监督学习算法不是利用带有正确答案的样本数据进行&#8221;训练&#8221;,他们的目的是要在一组数据中找寻某种结构,而这些数据本身并不是我们想要的答案.聚类算法的目标是采集数据,然后从中找出不同的群组.比如:非负矩阵因式分解和自组织映射.
为聚类算法准备数据的常见做法是定义一组公共的数值型属性.我们可以利用这些属性对数据项进行比较.
以博客用户为例.我们可以先定义一组指定的词汇,然后在博客中寻找出现的次数,通过单词出现的频度来对博客进行聚类..</p>

<pre lang="bash">
                  "china"         "kids"        "music"
Blog1             0                  3              3
Blog2             4                  0              3
</pre>


<p>数据可以到<a href="http://kiwitobes.com/clusters/blogdata.txt">这里</a>下载.
假设我们要自己搞定数据,我们需要对订阅源中的单词进行计数.
对rss源进行分析, 我们需要先下载feedparser模块.</p>

<pre lang="bash">
aptitude install python-feedparser
</pre>


<pre lang="python">
import feedparser
import re

"""
返回一个rss订阅源的标题和包含单词技术情况的字典.
"""
def getwordcount(url):
    #解析订阅源
    d = feedparser.parse(url)
    wc = {}

    for e in d.entries:
        if 'summary' in e:
            summary = e.summary
        else:
            summary = e.description

        #提取单词列表
        words = getwords(e.title+' '+summary)
        for word in words:
            wc.setdefault(word, 0)
            wc[word] += 1
    return d.feed.title,wc

"""
getwords把所有的html标记去掉,并以非字幕字符作为分隔符拆分出单词.
再将结果以列表形式加以返回.
"""
def getwords(html):
    txt = re.compile(r'<[^>]+>').sub('', html)
    words = re.compile(r'[^A-Za-z]+').split(txt)
    return [word.lower() for word in words if word!=' ']
</pre>


<p>接下来就是整体的调用过程.</p>

<pre lang="python">
apcount = {}
wordcounts = {}
feedlist = [line for line in file('feedlist.txt')]

for feedurl in feedlist:
    title,wc = getwordcounts(feedurl)
    wordcounts[title] = wc
    for word,count in wc.items():
        apcount.setdefault(word, 0)
        if count>1:
            apcount[word] += 1

wordlist = []
for w,bc in apcount.items():
    frac = float(bc)/len(feedlist)
    if frac>0.1 and frac<0.5:
        wordlist.append(w)

out = file('blogdata.txt', 'w')
out.write('Blog')
for word in wordlist:
    out.write('\t%s' % word)
out.write('\n')
for blog,wc in wordcounts.items():
    out.write(blog)
    for word in wordlist:
         if word in wc:
              out.write('\t%d' % wc[word])
         else:
              out.write('\t0')
    out.write('\n')
</pre>


<p>搞定数据后,我们开始分级聚类.
分级聚类就是通过连续不断的将最为相似的群组两两合并,来构造出一个群组的层级结构.其中每个群组都是从单一元素开始的.在每次迭带过程中,分级聚类算法会计算每两个群组间的距离,并将距离最近的两个群组合并成一个新的群组.这一过程会一直重复下去,直到只剩下一个群组为止.元素的相似程度是通过他们的相对位置来体现的,两个元素距离越近,他们就越相似.
以上面的数据为操作对象.</p>

<pre lang="python">
"""
首先,构造方法用来加载数据文件.
"""
def readfile(filename):
    lines = [line for line in file(filename)]

    #第一列是列标题.
    colnames = lines[0].strip().split('\t')[1:]
    rownames = []
    data = []
    for line in lines[1:]:
        p = line.strip().split('\t')
        #每行的第一列是列名
        rownames.append(p[0])
        data.append([float(x) for x in p[1:]])
    return rownames,colnames,data
</pre>


<p>下一步,我们来定义紧密度.我们根据拟合度来判断紧密度.</p>

<pre lang="python">
from math import sqrt
def pearson(v1, v2):
    sum1 = sum(v1)
    sum2 = sum(v2)
    sum1Sq = sum([pow(v,2) for v in v1])
    sum2Sq = sum([pow(v,2) for v in v2])
    pSum = sum([v1[i]*v2[i] for i in range(len(v1))])
    
    num = pSum-(sum1*sum2/len(v1))
    den = sqrt((sum1Sq-pow(sum1,2)/len(v1))*(sum2Sq-pow(sum2,2)/len(v1)))
    if den == 0: return 0
    return 1.0-num/den
</pre>


<p>上述1.0-num/den是为了让越相似的数据距离越小..
接下来,我们新建一个类,作为聚类.</p>

<pre lang="python">
class bicluster:
    def __init__(self, vec, left=None, right=None, distance=0.0, id=None):
         self.left = left
         self.right = right
         self.vec = vec
         self.id = id
         self.distance = distance
</pre>


<p>然后就是计算聚类的方法.</p>

<pre lang="python">
def hcluster(rows, distance=pearson):
    distances = {}
    currentclustid = -1

    #最开始的聚类就是数据集中的行.
    clust = [bicluster(row[i],id=i) for i in range(len(rows))]

    while len(clust)>1:
        lowestpair = (0,1)
        closet = distance(clust[0].vec, clust[1].vec)

        #便利每一个配对.寻找最小距离.
        for i in range(len(clust)):
            for j in range(i+1, len(clust)):
                #用distances来缓存距离的计算值
                if (clust[i].id, clust[j].id) not in distances:
                     distances[(clust[i].id, clust[j].id)] = distance(clust[i].vec, clust[j].vec)
                d = distances[(clust[i].id,clust[j].id)]
                if d<closet:
                     closet = d
                     lowestpair = (i,j)
       #计算两个聚类的平均值.
       mergevec = [
           (clust[lowestpair[0]].vec[i],clust[lowestpair[1]].vec[i])/2.0
           for i in range(len(clust[0].vec))]
       #建立新的聚类.
       newcluster = bicluster(mergevec, left=clust[lowestpair[0]],
                           right=clust[lowestpair[1]], distance=closet,
                           id=currentclustid)
       #不在原始集合中的聚类,其id为负数
       currentclustid -= 1
       del clust[lowestpair[0]]
       del clust[lowestpair[1]]
       clust.append(newcluster)
    return clust[0]
</pre>


<p>k-均值聚类.
分级聚类的结果为我们返回一棵形象直观的树,但这个方法有两个缺点:在没有额外的投入的情况下,树形视图是不会真正的将数据拆分成不同组的,而且该算法的计算量非常惊人,所以在大规模的数据处理的时候,该算法的运行速度会非常的慢.除了分级聚类外,另一种可供选择的聚类方法称为k-均值聚类.这种算法完全不同于分级聚类,因为我们会预先告诉算法希望生成的聚类数量,然后算法会根据数据的结构状况确定聚类的大小.
k-均值聚类算法首先会随机确定k个中心位置,然后各个数据项分配给最临近的中心点,待分配完成后,聚类中心就会移到分配给该聚类的所有节点的平均位置处,然后整个分配过程会重新开始.这一过程会一直重复下去,直到分配过程不再产生变化为止.</p>

<pre lang="python">
import random

def kcluster(rows, distance=pearson, k=4):
    #确定每个点的最小值和最大值.
    ranges = [(min([row[i] for row in rows]), max([row[i] for row in rows]))
                     for i in range(len(rows[0]))]

    #随机创建k个中心点.
    clusters = [[random.random()*(ranges[i][1]-ranges[i][0])+ranges[i][0]
                       for i in range(len(rows[0]))] for j in range(k)]

    lastmatches = None
    for t in range(100):
        print 'Iteration %d' % t
        bestmatches = [[] for i in range(k)]

        #在每一行中寻找距离最近的中心点.
        for j in range(len(rows)):
             row = rows[j]
             bestmatch = 0
             for i in range(k):
                 d = distance(cluster[i], row)
                 if d<distance(clusters[bestmatch], row):
                     bestmatch = i
             #如果结果与上一次相同, 则整个过程结束.
             if bestmatches == lastmatches:
                 break
             lastmatches = bestmatches
             #把中心点移到其所有成员的平均位置处.
             for i in range(k):
                 avgs = [0.0]*len(rows[0])
                 if len(bestmatches[i])>0:
                     for rowid in bestmatches[i]:
                         for m in range(len(rows[rowid]):
                             avgs[m] += rows[rowid][m]
                         for j in range(len(avgs)):
                             avgs[j] /= len(bestmatches[i])
                         clusters[i] = avgs
      return bestmatches
</pre>

</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">wangxiaomo</span></span>

      








  


<time datetime="2011-07-13T03:08:00+08:00" pubdate data-updated="true">Jul 13<span>th</span>, 2011</time>
      

<span class="categories">
  
    <a class='category' href='/oct/blog/categories/集体智慧/'>集体智慧</a>
  
</span>


    </p>
    
      <div class="sharing">
  
  <a href="http://twitter.com/share" class="twitter-share-button" data-url="http://wangxiaomo.github.com/blog/2011/07/13/%E6%95%B0%E6%8D%AE%E8%81%9A%E7%B1%BB/" data-via="" data-counturl="http://wangxiaomo.github.com/blog/2011/07/13/%E6%95%B0%E6%8D%AE%E8%81%9A%E7%B1%BB/" >Tweet</a>
  
  
  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/oct/blog/2011/07/13/Tanimoto%E5%88%86%E5%80%BC%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97/" title="Previous Post: Tanimoto分值相似度计算">&laquo; Tanimoto分值相似度计算</a>
      
      
        <a class="basic-alignment right" href="/oct/blog/2011/07/15/%E4%B8%AD%E6%96%87%E5%88%86%E8%AF%8D.mmseg%E8%BF%90%E7%94%A8.%60/" title="Next Post: 中文分词.mmseg运用.`">中文分词.mmseg运用.` &raquo;</a>
      
    </p>
  </footer>
</article>

</div>

<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/oct/blog/2012/06/11/emacs+auto-complete+plugins/">emacs auto-complete plugins</a>
      </li>
    
      <li class="post">
        <a href="/oct/blog/2012/06/09/LaTex+CJK+%E5%AD%97%E4%BD%93%E9%97%AE%E9%A2%98%5BLinux%5D/">LaTex CJK 字体问题[Linux]</a>
      </li>
    
      <li class="post">
        <a href="/oct/blog/2012/06/08/nmap+%E4%BD%BF%E7%94%A8./">nmap 使用.</a>
      </li>
    
      <li class="post">
        <a href="/oct/blog/2012/06/07/Arch+PGP+xxx+unknown+%E8%A7%A3%E5%86%B3/">Arch PGP xxx unknown 解决</a>
      </li>
    
      <li class="post">
        <a href="/oct/blog/2012/06/04/Mongo+%E8%81%9A%E5%90%88/">Mongo 聚合</a>
      </li>
    
  </ul>
</section>






  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2012 - wangxiaomo -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  







  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>





</body>
</html>
